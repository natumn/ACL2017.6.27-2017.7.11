{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "story-classifiy-doc2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1vfAXuBBRHkbMk5bpuEHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natumn/ACL2017.6.27-2017.7.11/blob/master/story_or_not_classifiy_doc2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTxBkYDVjM9f",
        "colab_type": "text"
      },
      "source": [
        "## 前処理モデル生成の前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTnA4oOFifVT",
        "colab_type": "text"
      },
      "source": [
        "Cloud Storageにあるストーリーのデータにアクセスする\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKTE9gHGjWwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "import json\n",
        "from json.decoder import WHITESPACE\n",
        "\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project tellerapi-dev\n",
        "!mkdir /content/data\n",
        "!gsutil cp gs://natumn-dev/story-data/000000000000 /content/data/story-data0.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWCcagh6ILGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "storyFile = open('/content/data/storyStrings.txt', 'w')\n",
        "\n",
        "with open('/content/data/story-data0.json', encoding='utf-8') as f:\n",
        "  counter = 0\n",
        "  for line in f:\n",
        "      counter += 1\n",
        "      storyStr = '\\n#story{0}\\n'.format(counter)\n",
        "      data = json.loads(line)\n",
        "      if data.get('Script') != '' and data.get('Script') != None:\n",
        "        scriptData = json.loads(data.get('Script'))\n",
        "        if scriptData.get('sceneList') != '':\n",
        "          for scene in scriptData['sceneList']:\n",
        "            if scene.get('ops') != '' and scene.get('ops') != None:\n",
        "              for op in scene.get('ops'):\n",
        "                if op.get('postMessageOp') != '' and op.get('postMessageOp') != None:\n",
        "                  postMessageOp = op.get('postMessageOp')\n",
        "                  if postMessageOp.get('text') != None:\n",
        "                    storyStr += postMessageOp.get('text') + ' '\n",
        "                elif op.get('showTextOp') != '' and op.get('showTextOp') != None:\n",
        "                  showTextOp = op.get('showTextOp')\n",
        "                  if showTextOp.get('text') != None:\n",
        "                    storyStr += showTextOp.get('text') + ' '\n",
        "\n",
        "      storyFile.write(storyStr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWWhFHED5Yp8",
        "colab_type": "text"
      },
      "source": [
        "## Doc2Vecを使った文書類似度算出を行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1WQ7Q45dbp",
        "colab_type": "text"
      },
      "source": [
        "Doc2Vecをgensimというライブラリを利用から使う。\n",
        "\n",
        "[gensimについて](https://radimrehurek.com/gensim/)\n",
        "\n",
        "[gensim入門（Qiitaの記事）](https://qiita.com/u6k/items/5170b8d8e3f41531f08a)\n",
        "\n",
        "まずはgensimをインストールする。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIsssRi7NaJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5JLKU4Q4zt5",
        "colab_type": "text"
      },
      "source": [
        "形態素解析システムのJUMAN++をインストールする \n",
        "\n",
        "[JUMAN++について](http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN++)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XhfEZTbOMu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.01.tar.xz\n",
        "!tar -Jxvf jumanpp-1.01.tar.xz\n",
        "%cd jumanpp-1.01 \n",
        "!./configure\n",
        "!make\n",
        "!sudo make install\n",
        "!jumanpp -v\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWpxSf-263jP",
        "colab_type": "text"
      },
      "source": [
        "日本語構文解析システムのKNPをインストールする\n",
        "\n",
        "[KNPについて](http://nlp.ist.i.kyoto-u.ac.jp/?KNP)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2xMCrJi66jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/knp/knp-4.19.tar.bz2\n",
        "!tar xf knp-4.19.tar.bz2\n",
        "%cd knp-4.19\n",
        "!./configure\n",
        "!make\n",
        "!sudo make install\n",
        "!echo \"knpとjumanを組み合わせる\" | jumanpp | knp\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5mKTfMH9kB-",
        "colab_type": "text"
      },
      "source": [
        "KNPをPython上から実行するためKNPのPythonバインディングをインストールする\n",
        "\n",
        "[ソース](http://nlp.ist.i.kyoto-u.ac.jp/index.php?PyKNP)\n",
        "\n",
        "[Github](https://github.com/ku-nlp/pyknp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETN0qDpA91CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/knp/pyknp-0.3.tar.gz\n",
        "!tar zxvf pyknp-0.3.tar.gz\n",
        "%cd pyknp-0.3\n",
        "!python setup.py install\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlVPQsESsOFZ",
        "colab_type": "text"
      },
      "source": [
        "Doc2Vecで文章を学習させる "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuqbuspIsMce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from os import listdir, path\n",
        "from pyknp import Jumanpp\n",
        "from gensim import models\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "\n",
        "def corpus_files():\n",
        "    dirs = [path.join('./text', x)\n",
        "            for x in listdir('./text') if not x.endswith('.txt')]\n",
        "    docs = [path.join(x, y)\n",
        "            for x in dirs for y in listdir(x) if not x.startswith('LICENSE')]\n",
        "    return docs\n",
        "\n",
        "def read_document(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "def split_into_words(text):\n",
        "    result = Jumanpp().analysis(text)\n",
        "    return [mrph.midasi for mrph in result.mrph_list()]\n",
        "  \n",
        "def doc_to_sentence(doc, name):\n",
        "    words = split_into_words(doc)\n",
        "    return LabeledSentence(words=words, tags=[name])\n",
        "\n",
        "def corpus_to_sentences(corpus):\n",
        "    docs   = [read_document(x) for x in corpus]\n",
        "    for idx, (doc, name) in enumerate(zip(docs, corpus)):\n",
        "        sys.stdout.write('\\r前処理中 {}/{}'.format(idx, len(corpus)))\n",
        "        yield doc_to_sentence(doc, name)\n",
        "  \n",
        "corpus = corpus_files()\n",
        "sentences = corpus_to_sentences(corpus)\n",
        "\n",
        "model = models.Doc2Vec(sentences, dm=0, size=300, window=15, alpha=.025,\n",
        "        min_alpha=.025, min_count=1, sample=1e-6)\n",
        "\n",
        "print('\\n訓練開始')\n",
        "for epoch in range(20):\n",
        "    print('Epoch: {}'.format(epoch + 1))\n",
        "    model.train(sentences)\n",
        "    model.alpha -= (0.025 - 0.0001) / 19\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save('doc2vec.model')\n",
        "model = models.Doc2Vec.load('doc2vec.model')\n",
        "\n",
        "# model.docvecs.similarity('./text/livedoor-homme/livedoor-homme-4700669.txt', './text/movie-enter/movie-enter-5947726.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}